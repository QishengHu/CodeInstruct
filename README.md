# CodeInstruct: Empowering Language Models to Edit Code

![Alt text](./fig/demo.png "Pipeline & Example")

## This repo hosts the dataset, code for the CodeInstruct project. The project presents the CodeInstruct dataset designed to adapt language models (LLMs) for code editing. 


## Please check out [our blog](https://blog.nus.edu.sg/kaixinli/2023/05/23/codeinstruct/) with more details. 

## You can also read [our paper]().

-------------------
# Overview
CodeInstruct is the first dataset designed to adapt LLMs for general code editing. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios, generated by ChatGPT. 



# Data Collection
To generate instructional data for code editing, we employed a similar method to Self-Instruct. This methodology of generating training data using LLMs requires minimal human-labeled data as seed tasks while still maintaining the quality and relevance of the tasks in the dataset. CodeInstruct is systematically expanded through an iterative process that commences with editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently bootstrapped to prompt ChatGPT for more task data. 


# Results and Examples
The largest model fine-tuned, LLaMA-33B, performs on par with ChatGPT, with 79.3% accuracy on the test set. Some qualitative examples are shown below:

1. 
Instruction:

# Release
We are planning to release the following assets:
- [x] Full dataset: Over 114,000 code-editing instructional data.
- [x] Train / Validation data: Arround 95% / 5% of the full dataset.
- [x] Test data: 134 github commit data.
- [x] Github seed data: A total of 634 github commit data.
- [x] Additional seed data: 592 unused generated samples.
- [ ] TODO: Source Code
- [ ] TODO: Checkpoints

We are currently working on a clean release of our code. We will upload the code as soon as we finish the job.

We also plan to release our LoRA checkpoints [here](https://huggingface.co/happylkx).